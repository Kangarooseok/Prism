# 📡 PRISM — CCTV 상태 자동화 관제 플랫폼 README

<img width="1920" height="1080" alt="Prism" src="https://github.com/user-attachments/assets/62876cda-6637-4e1d-a945-e4d0ea3a18ea" />


## 접속/데모 정보

- **배포 URL**: 준비 중 *(사내 온프레미스 운영, 비용 이슈로 현재 서버 중지 상태)*

### Demo 계정
| 역할 | ID | Password |
|---|---|---|
| 관리자 | `admin@prism.com` | `password123` |
| 네트워크 관리자 | `network@prism.com` | `password123` |
| 장치 관리자 | `operator@prism.com` | `password123` |

> 보안 안내: 데모 계정은 **조회(Read) 권한만 부여**됩니다. 쓰기/삭제/설정 변경은 차단됩니다.  

- **문의**: [kms860926@gmail.com](mailto:kms860926@gmail.com)

<br>

## 프로젝트 소개

  <img width="756" height="476" alt="카툰" src="https://github.com/user-attachments/assets/37d93f11-ad22-42af-a3f7-c3e282fb6d12" />
  <br>


### 배경
- 기존 관제는 사람이 화면을 지켜보며 장애를 찾는 방식이었습니다. 교대 근무나 야간 시간대에는 인지 지연과 누락이 잦고, 장애가 실제로 있었는지 입증할 기록이 부족했습니다.
- 팀은 사람의 주의력에 기대기보다 **시스템이 스스로 감지·알림·기록**하는 흐름을 만들고자 했고, 소규모 설비와 하이브리드 환경을 직접 꾸려 **실시간 관제**를 검증하는 것을 목표로 삼았습니다.

### 문제 정의
- **인지 지연**: 장애가 발생해도 알림이 늦거나 놓치는 경우가 있었습니다.  
- **기록의 공백**: 언제 어떤 장애가 있었는지, 누가 조치했는지 한눈에 보기 어려웠습니다.  
- **서비스 간 결합**: 여러 서비스가 서로의 저장소를 직접 참고해 변경에 취약했습니다.  
- **배포·운영 혼선**: 프런트와 백엔드 주소 체계가 달라 CORS와 혼합 콘텐츠 문제가 반복됐습니다.  
- **비용과 복구**: 클라우드 단일 전략만으로는 비용이 커지고, 장애 대비 시나리오가 제한적이었습니다.

### 해결 방안
- **이벤트 중심 구조**: Kafka를 허브로 삼아 서비스 간 연결을 느슨하게 만들었습니다.  
- **조회 분리와 동기화**: 쓰기/읽기를 분리하고 Debezium으로 변경을 스트림으로 흘려 **조회 모델을 신속 동기화**했습니다.  
- **상태 감지와 판별**: 핑, 스트리밍 세션, 응답 시간·손실률을 주기 수집하고, **간단한 규칙**으로 네트워크/장비 이슈를 분리했습니다.  
- **역할 기반 화면**: 관리자, 네트워크 담당자, 장치 담당자 각각의 시작 화면과 메뉴를 분리했습니다.  
- **하이브리드 운영**: 상시는 온프레미스, 확장·복구는 클라우드로 나눠 비용과 안정성을 함께 가져갔습니다.  
- **배포 표준**: 프런트는 항상 **`/api` 상대 경로**로 호출하고, 게이트웨이에서 백엔드로 라우팅하도록 규칙을 통일했습니다.

### 결과
- **수집 주기**: 10초 간격으로 상태를 모아 대시보드에 바로 반영했습니다.  
- **네트워크 지표**: 시험 환경에서 평균 응답 시간 **16~18ms**, 최대 **29ms** 수준이었고, 스파이크는 그래프에서 즉시 확인할 수 있었습니다.  
- **대시보드 요약 카드**: 전체 수량·온라인·장애·주의를 카드로 묶고 **전일 대비 증감**까지 함께 보여 상황 파악 시간을 줄였습니다.  
- **알림·이력**: 최근 알림 10건 요약, 그룹/담당자/검색 필터, 심각도 배지로 **조치 이력 추적**이 빨라졌습니다.  
- **운영 품질**: `/api` 프록시 표준화 이후 CORS와 경로 이슈가 사실상 사라졌고, 배포 절차가 단순해졌습니다.  
- **복구 전략**: 온프레미스와 클라우드를 나눠 쓰면서 목표 복구 시간과 데이터 손실 목표를 맞출 수 있는 가능성을 확인했습니다.

<br>

> [CCTV 장애대응 시연영상](https://youtu.be/xXVvPPtkZEQ)


<br>

## 개발 인원

<div align="center">

| **강민석** | **김시현** | **최현제** | **한선영** |
| :------: |  :------: | :------: | :------: |
| [<img src="https://github.com/user-attachments/assets/6e7bdf44-e4eb-4ca1-bf29-7fc73e955458" height=150 width=150> <br/> @kangroosek](https://github.com/Kangarooseok) | [<img src="https://github.com/user-attachments/assets/bb1c3856-ebad-4896-9065-5938d628d568" height=150 width=150> <br/> @kimsihyon](https://github.com/kimsihyon) | [<img src="https://github.com/user-attachments/assets/bebe11d2-b96b-4dcf-81fc-2e6645f05a91" height=150 width=150> <br/> @choihyunjae](https://github.com/) | [<img src="https://github.com/user-attachments/assets/778c9b04-d272-4f98-b459-6c2177521888" height=150 width=150> <br/> @HanSeonyoung](https://github.com/HanSeonyoung) |

</div>

<br>

## 1. 개발 환경

- **Front** : React, Vite, Recharts, HLS.js, Tailwind/Lucide
- **Back-end** : Spring Boot (Web, Validation, Scheduler), Spring Data JPA, Drools
- **Stream/DB** : Kafka, Kafka Connect, Debezium CDC, MariaDB (쓰기/읽기 분리)
- **Infra** : Kubernetes(EKS/온프레), Proxmox, RAID, Galera Cluster, MaxScale, Vault
- **보안/네트워크** : Fortigate IPSec Site‑to‑Site VPN, VLAN, VPC
- **형상/협업** : GitHub (Projects/Issues), Jira, Notion, Slack, GoogleDrive
  
<br>

## 2. 채택한 개발 기술과 브랜치 전략

### EDA + Kafka, CQRS + CDC
- 서비스 간 **느슨한 결합**과 **무중단 확장**을 위해 Kafka를 이벤트 허브로 사용.  
- **CQRS**로 쓰기/읽기 DB 분리, **Debezium CDC**로 변경 이벤트를 스트림화해 조회 성능과 일관성 확보.

### Rule Engine & 상태 감지
- ICMP/RTSP 지표(응답/RTT/손실/세션)를 **10초 주기**로 수집하고 **Drools**로 진단 코드·심각도 부여.

### 브랜치 전략
- **main / develop / feature** 기반 Git‑flow 변형. 기능 단위 PR → CI 빌드 → 통합 테스트 후 병합.

<br>

## 3. 프로젝트 구조

```
prism/
├── backend/
│   ├── controller/
│   │   ├── .gradle/
│   │   ├── .idea/
│   │   ├── .vscode/
│   │   ├── aws/
│   │   ├── build/
│   │   ├── gradle/
│   │   ├── kubernetes/
│   │   │   ├── db-config.yaml
│   │   │   ├── db-secret.yaml
│   │   │   ├── deployment.yaml
│   │   │   └── service.yaml
│   │   └── src/
│   │       └── main/
│   │           └── java/
│   │               └── prism/
│   │                   ├── config/
│   │                   │   ├── ErrorDbConfig.java
│   │                   │   ├── JpaTxConfig.java
│   │                   │   ├── MainDbConfig.java
│   │                   │   ├── SubscriptionDbConfig.java
│   │                   │   ├── UserDbConfig.java
│   │                   │   └── WebConfig.java
│   │                   ├── domain/
│   │                   │   ├── cctv/
│   │                   │   │   ├── command/
│   │                   │   │   │   ├── DeleteCctvCommand.java
│   │                   │   │   │   ├── ModifyCctvCommand.java
│   │                   │   │   │   └── RegisterCctvCommand.java
│   │                   │   │   ├── model/
│   │                   │   │   │   └── Cctv.java
│   │                   │   │   └── repository/
│   │                   │   │       ├── CctvErrorStatusRepository.java
│   │                   │   │       └── CctvRepository.java
│   │                   │   ├── group/
│   │                   │   │   ├── command/
│   │                   │   │   │   ├── DeleteCctvGroupCommand.java
│   │                   │   │   │   ├── ModifyCctvGroupCommand.java
│   │                   │   │   │   └── RegisterCctvGroupCommand.java
│   │                   │   │   ├── model/
│   │                   │   │   │   └── CctvGroup.java
│   │                   │   │   └── repository/
│   │                   │   │       └── CctvGroupRepository.java
│   │                   │   ├── subscription/
│   │                   │   │   └── repository/
│   │                   │   │       └── SubscriptionDao.java
│   │                   │   └── user/
│   │                   │       ├── dto/
│   │                   │       │   └── UserDto.java
│   │                   │       └── repository/
│   │                   │           └── UserReadDao.java
│   │                   └── infra/
│   │                       ├── bootstrap/
│   │                       │   └── UnassignedBackfillRunner.java
│   │                       └── controller/
│   │                           ├── CctvController.java
│   │                           ├── CctvDiagnosisController.java
│   │                           ├── CctvGroupController.java
│   │                           ├── CctvStatsController.java
│   │                           ├── HealthCheckController.java
│   │                           ├── SubscriptionsController.java
│   │                           └── UsersController.java
│   ├── status_check/           # 파일 아직 없음
│   ├── error_detection/        # 파일 아직 없음
│   └── LoginBackend/           # 파일 아직 없음
└── frontend/
    ├── dist/
    ├── frontend-kubenetes/
    │   ├── frontend-deployment.yaml
    │   ├── frontend-ingress.yaml
    │   └── frontend-service.yaml
    ├── node_modules/
    ├── public/
    │   ├── assets/
    │   │   └── images/
    │   │       ├── Prism_dark.svg
    │   │       └── Prism_light.svg
    │   └── index.html
    ├── src/
    │   ├── assets/
    │   │   ├── PrismDarkLogo.jsx
    │   │   └── PrismLightLogo.jsx
    │   ├── components/
    │   │   ├── CCTVGroupManagement.jsx
    │   │   ├── CCTVManagement.jsx
    │   │   ├── CctvPlayer.jsx
    │   │   ├── Dashboard.jsx
    │   │   ├── LoginPage.jsx
    │   │   └── UserManagement.jsx
    │   ├── app.js
    │   ├── index.css
    │   └── index.js
    ├── .babelrc
    ├── .env.development
    ├── .env.production
    ├── .gitignore
    ├── Dockerfile
    ├── package-lock.json
    ├── package.json
    ├── postcss.config.js
    ├── README
    ├── tailwind.config.js
    └── webpack.config.js

```

<br>

## 4. 역할 분담

### 🍊강민석

- 프런트엔드 총괄: UI 아키텍처·라우팅 설계, 대시보드/관리 화면, 상태·이력 시각화, 빌드(Webpack/Vite) E2E 구현
- 기획/PRD 주도: 감지→판별→알림→복구→기록 엔드투엔드 흐름 설계, 무중단 관제·책임추적 목표에 맞춘 범위/우선순위 확정
- 대시보드·지표: 상태 집계 API 연동, 전일 대비 증감률/비율 계산, 분모 0 예외 처리 및 UI 반영
- CCTV 관리(CRUD): 등록·수정·삭제·조회, DTO/Validation, 그룹 연동, 소프트 삭제(@SQLDelete) 적용
- 그룹 관리: 생성·수정·삭제, ‘미정’ 그룹 보호, 배정/해제 diff 로직 및 일괄 이동 처리
- 사용자/권한: 사용자 CRUD, 역할 기반(관리자/네트워크/운영) 화면·액션 제어
- 구독(알림 대상): CCTV·그룹 ↔ 담당자 구독 생성/해제, 중복·충돌 방지 검증
- 스트리밍: Hls.js 플레이어 래퍼(치명/비치명 오류 분류, 자동 복구·수동 재시도, 상태 오버레이)
- 배포(앱 레벨): Docker 빌드 → K8s 매니페스트(Deployment/Service/ConfigMap/Secret) 작성/적용, readiness/liveness 프로브, DB 연결 설정 분리
- 라우팅 구성(앱 레벨): Ingress + ExternalName Service로 FE–BE 분리, /api 프록시·리라이트 규칙 표준화 → 운영 동선 단순화

<br>
    
### 👻김시현 (차후에 작성예정)

- 상태 감지 백엔드 개발
- 데이터모델링
- CQRS  설계

<br>

### 😎최현제 (차후에 작성예정)

- Kubernetes 구축:
- Kafka-CQRS 구축:
- 네트워크/보안:
- 성능/비용 최적화:
- DR 디자인:

<br>

### 🐬한선영 (차후에 작성예정)

- 상태 감지 백엔드 개발
- 알림 백엔드 개발
- 시스템 설계
- 데이터모델링
- 일정 관리
    
<br>

## 5. 개발 기간 및 작업 관리

### 전체 개발 기간 : 2025.07.21(월) ~ 2025.09.12(금) · 총 8주
<br>

### 작업 관리 : GitHub Projects & Issues(칸반) · 데일리 스탠드업(15분) · 주간 리캡(리스크/일정 조정)
<br>

### 주차 구간(참고)
- W1: 07/21–07/27 · W2: 07/28–08/03 · W3: 08/04–08/10 · W4: 08/11–08/17
- W5: 08/18–08/24 · W6: 08/25–08/31 · W7: 09/01–09/07 · W8: 09/08–09/12
<br>

### 일정(간트 요약)

> 블록(█)이 해당 주에 수행된 작업을 의미합니다.

| 프로젝트 항목         |  W1 |  W2 |  W3 |  W4 |  W5 |  W6 |  W7 |  W8 | 기간(날짜)      |
| --------------- | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | ----------- |
| 프로젝트 계획/분석      |  █  |     |     |     |     |     |     |     | 07/21–07/25 |
| 프로젝트 설계         |  █  |  █  |     |     |     |     |     |     | 07/23–07/31 |
| 프로젝트 개발(S/W)    |     |  █  |  █  |  █  |     |     |     |     | 07/28–08/17 |
| 인프라 구축          |     |     |  █  |  █  |  █  |  █  |     |     | 08/04–08/31 |
| 각 기능 테스트        |     |     |     |  █  |  █  |  █  |  █  |     | 08/11–09/07 |
| 내부 발표회 및 산출물 준비 |     |     |     |    |  █  |  █  |    |     | 08/18–08/29 |
| 최종 산출물 정리       |     |     |     |    |    |  █  |  █  |  █  | 08/31–09/12 |
| 성과발표회 및 시연 준비 |     |     |     |     |    |    |  █  |  █  | 09/01–09/12 |


<br>

## 6. 신경 쓴 부분

### [인프라]
- **하이브리드 DR**: 온프레미스 Active ↔ AWS Passive 자동 전환 시나리오
- **보안/망 구성**: IPSec **Site‑to‑Site VPN**, 내부 VLAN 분리, VPC 서브넷 구조
- **운영 표준화**: PXE Boot + Kickstart로 OS 설치 자동화, 환경·경로 규칙 일원화
- **가용성·일관성**: Galera Cluster + MaxScale, CDC 기반 읽기 모델 동기화

### [백엔드]
-

### [프론트]
-

<br>

## 7. 페이지별 기능

### [로그인]
- 서비스 접속 초기화면입니다. 이메일/비밀번호를 입력하면 로그인 버튼이 활성화됩니다.
- 인증 실패 시 입력창 하단에 경고 문구가 노출됩니다.
- 인증 성공 시 **사용자 역할(Role)**에 따라 서로 다른 대시보드로 이동합니다.
- 하단의 비밀번호를 잊으셨나요? 링크를 통해 재설정 플로우로 진입합니다.
  <br>
- 권한(역할) 및 초기 랜딩

| 역할                            | 랜딩 페이지          | 주요 메뉴 노출                                          | 권한 범위(예시)                                          |
| ----------------------------- | --------------- | ------------------------------------------------- | -------------------------------------------------- |
| **관리자 (admin)**               | 관리자 대시보드        | 대시보드 · CCTV 그룹 관리 · CCTV 관리 · CCTV 장애 이력 · 사용자 관리 | 전 메뉴 **읽기/쓰기** · 사용자/권한 관리 · 장애이력 확인                 |
| **네트워크 담당자 (network\_admin)** | 네트워크/장애 중심 대시보드 | 대시보드                | 네트워크 상태 모니터링, 장애이력 확인           |
| **장치 담당자 (operator)**         | 장치 운영 대시보드      | 대시보드                                    | 일정관리, 장애 원인 기록 |


| 로그인 |
|----------|
|<img width="1680" height="930" alt="로그인화면" src="https://github.com/user-attachments/assets/360dd97a-df7e-4bca-98b6-e358c5fba41f" />|



<br>

### [관리자 대시보드]
- 좌측 사이드바:
  · 대시보드
  · CCTV 그룹 관리
  · CCTV 관리
  · CCTV 장애 이력
사용자 관리 메뉴가 고정 노출됩니다.
하단에는 관리자 계정 정보, 다크모드 토글, 로그아웃 버튼이 있습니다.

- 상단 요약 카드 4종:
  · 전체 CCTV(예: 4대, 전일 대비 +33.3%)
  · 온라인 상태(예: 2대, -50.0%)
  · 장애 발생(예: 1건, +100.0%)
  · 주의 필요(예: 1건, +100.0%)
각 카드 우측 상단에 증감 퍼센트가 화살표와 함께 표시됩니다.

- 실시간 CCTV 목록:
  중앙 영역에 썸네일 그리드가 표시되며, 각 카드에 상태 배지(예: ACTIVE, OFFLINE, WARNING)와 위치 라벨(예: ALPHA 강의실, 서버실1) 및 IP가 함께 표시됩니다.
  상단에는 **총 대수(예: 총 4대)**와 LIVE 녹색 표시가 노출됩니다. OFFLINE인 경우 카메라 아이콘과 함께 화면이 비활성 상태로 표시됩니다.

- 우측 장애 알림 패널:
  실시간 알림 리스트가 심각도 배지(예: 위험, 주의)와 영역 태그(예: 강의실, 서버실)로 구분되어 표시됩니다.
  각 항목은 발생 경과 시간(예: 4분 전)과 담당자(예: 강민석, 이상은) 정보를 함께 보여주며, 상단에는 **활성 알림 수(예: 3 활성)**가 표시됩니다.

상단 글로벌 알림: 우측 상단 종 아이콘에 읽지 않은 알림 수(예: 3)가 뱃지로 표시됩니다.

| 관리자 대시보드 |
|----------|
|<img width="1680" height="930" alt="관리자 대시보드" src="https://github.com/user-attachments/assets/6ca43ed4-6bc6-41a6-b0e1-2e92d5d07726" />|


<br>

### [CCTV 그룹관리]
- 그룹 패널:
  · 그룹 검색 입력창과 그룹 생성 버튼이 상단에 있고, 아래로 그룹 리스트가 표시됩니다.
  · 각 항목에는 **그룹명과 설명, CCTV 수, 담당자 수, 등록일자가 함께 보여요.

- 미정 그룹:
  · 잠금 아이콘과 함께 표시되며, 이름 변경/삭제가 비활성화됩니다.
  · 미정 그룹에 배정되지 못한 CCTV가 있으면 좌측메뉴과 cctv 그룹관리 페이지에 알림이 뜹니다.

- 그룹 수정:
  · 그룹명과 설명, 편집 · 삭제 버튼이 위치합니다.
  · 그룹 삭제 및 cctv 제외 시엔 소속 CCTV를 자동으로 ‘미정’ 그룹으로 이동합니다.
  · 이미 다른 그룹에 속한 CCTV, 담당자는 목록에서 안보이며, 오직 미정 그룹에만 있는 CCTV, 담당자만 선택 가능하도록 예외 처리됩니다.



| CCTV 그룹관리 (gif 용량 압축 과정 중에 화면 색이 변색됨) |
|----------|
|![그룹관리페이지](https://github.com/user-attachments/assets/84f2d93a-97d5-45fd-84c4-51c306c6a765)|

<br>


### [CCTV 관리]
- CCTV 리스트:
  · 정렬(장치명, IP주소, 설치위치, 상태)를 제공해 빠르게 대상을 좁힙니다.
  · 우측에는 CCTV 등록 버튼이 있어 신규 장비를 추가할 수 있습니다.
  · 각 CCTV 요소들은 함께 상태 배지(ACTIVE, WARNING, OFFLINE)가 우측에 배치되어있습니다.

- 세부 패널:
  · 각 CCTV를 클릭하면 상세 정보가 열리며, 편집 · 삭제 버튼과 함께 장치명, IP주소, 설치위치, 상태, 도로명주소, HLS주소의     속성을 확인·수정할 수 있습니다.

- 삭제(소프트 삭제):
  · 삭제 시 레코드는 @SQLDelete 기반 소프트 삭제로 목록에서 제외됩니다. 복구가 필요한 경우 관리자만 별도 API로 복원합니다.

- 실시간 반영:
  · 등록/수정/삭제은 성공/실패 토스트로 안내되며, 대시보드와 그룹관리 페이지에 즉시 리프레시됩니다.



| CCTV 관리 (gif 용량 압축 과정 중에 화면 색이 변색됨) |
|----------|
|![CCTV 관리](https://github.com/user-attachments/assets/595ff1d7-f9e0-4474-856c-c1779393847b)|


<br>

### [CCTV 장애이력 & 알림이력]
[CCTV 장애 이력]
- 목록 테이블:
  · 순번
  · 장애 판단 기준(예: 응답없음/지연/패킷 손실)
  · 장애 심각도
  · CCTV명
  · CCTV 그룹명
  · 장애 발생일시
  · 담당자 순으로 표시됩니다.

- 심각도 배지:
  · 위험/주의가 컬러 배지로 구분되어 한눈에 중대도를 파악할 수 있습니다.

- 정렬/표시 방식:
  · 기본은 발생일시 최신순으로 정렬되어 최근 장애를 우선 확인할 수 있습니다.

- 담당자 정보
  · 각 행에 담당자 이름이 함께 표기되어, 후속 조치자 확인이 즉시 가능합니다.
  
[알림 이력]
- 모달 뷰:
  · 대시보드에서 호출되는 오버레이 모달로, 최근 알림을 테이블로 제공합니다.

- 상단 필터 3종:
  · 검색(키워드) · 그룹 선택(드롭다운) · 담당자 선택(드롭다운) 으로 원하는 알림만 빠르게 필터링할 수 있습니다.

- 테이블 컬럼:
  · 그룹명
  · CCTV명
  · 담당자
  · 오류 유형(예: 응답 없음/네트워크 지연/패킷 손실)
  · 발생 일시
  · 심각도

요약 범례: 하단에 심각/오류/경고 개수 요약과 “최근 10건만 표시됩니다.” 안내가 함께 노출됩니다.

| CCTV 장애이력 & 알림이력 (gif 용량 압축 과정 중에 화면 색이 변색됨) |
|----------|
|![장애이력_알림이력](https://github.com/user-attachments/assets/b963d961-e17c-4af2-b7b1-dfd3aa689fbe)|

<br>


### [사용자 관리]

- 사용자 등록 :
  · 상단 우측 사용자 등록 버튼으로 신규 사용자를 추가합니다.
  · 입력 항목:
      · 이름
      · 이메일
      · 권한 (관리자/네트워크 관리자/장치 관리자)
      · 상태 (활성/비활성)
      · 비밀번호
      · 비밀번호 확인
  · 필수값 미입력/형식 오류(예: 이메일)는 필드 하단에 검증 메시지로 안내됩니다.
  · 등록 시 목록에 즉시 반영되고 성공/실패 토스트가 노출됩니다.

- 사용자 상세 조회 :
  · 읽기 전용으로 이름/이메일/권한/상태와 등록일(읽기전용) · 마지막 로그인(읽기전용)을 확인할 수 있습니다.
  · 하단의 삭제 버튼으로 사용자를 제거할 수 있으며, 확인 모달 이후 처리됩니다.

- 사용자 수정 :
  · 이름/이메일/권한/상태를 변경 후 저장합니다.
  · 비밀번호 변경은 별도 절차(재설정 플로우)를 사용합니다.


| 사용자 관리 (gif 용량 압축 과정 중에 화면 색이 변색됨) |
|----------|
|![사용자 관리](https://github.com/user-attachments/assets/d7dbc3a7-f77b-4b0e-b0f8-5620882c7ab6)|


<br>

### [네트워크 담당자 대시보드]

- 네트워크 대시보드 :
  · 좌측 사이드바에서 CCTV 네트워크 모니터링을 선택하면 중앙에 카메라 카드 그리드가 나열됩니다.
  · 각 카드에는 상태 배지, 실시간 영상, 장치명, IP, 패킷 손실률, 대역폭, 마지막 확인 시각이 표시되어 네트워크 컨디션을 한눈에 파악할 수 있습니다.

- CCTV 네트워크 모니터링 화면
  · 카드를 클릭하면 상세 화면으로 이동합니다. 상단에 카메라 이름과 현장 주소가 노출됩니다.
  · 우측 네트워크 차트표에서, 기간 선택, 자동 새로고침 체크, 새로고침 버튼으로 실시간 모니터링을 제어합니다.
  · 우측 패널은 네트워크 상태 차트로, PACKET LOSS(%) · RTT AVG · RTT MAX · RTT MIN · STATUS 지표가 시간축에 선형으로 표시됩니다.
  · 차트 상단에는 응답률(예: 99.2%) 과 대역폭(예: 10 Mbps) 요약이 함께 표기됩니다.
  · 좌측 패널은 실시간 영상으로 LIVE/REC 배지와 함께 스트림을 즉시 확인합니다.
  · 손실/지연 스파이크가 감지되면 경고 색으로 드러나 원인 추적을 돕습니다.

- 상세 모니터링 데이터(테이블)
  · 화면 하단 상세 모니터링 데이터는 측정 시점별 100개의 로그를 제공합니다.
  · 기본 컬럼: 요청시간 · 상태(SUCCESS/FAIL) · 최대RTT · 평균RTT · 최소RTT · 패킷손실률.


| 네트워크 담당자 대시보드 |
|----------|
|![네트워크 대시보드](https://github.com/user-attachments/assets/bac21adc-fd99-4a65-a103-e55c93aaf11c)|

<br>

### [장치 담당자 대시보드] (UI 프로토타입)

- 목적:
  · 장치 담당자가 정비 일정·도구 재고·장애 통계를 한 곳에서 보고, 일정/조치까지 관리하도록 설계된 전용 대시보드입니다.
  · 현재는 UI만 구현되어 있으며, 주요 버튼/목록은 더미 상태입니다.

- 좌측 사이드바:
  · 대시보드
  · 정비 일정 목록
  · 정비 도구 목록
  · 일정 예약 관리
  · 장애 통계/리포트
  메뉴로 구성되어 향후 각 기능 화면으로 이동합니다.

- 상단 요약 카드(지표):
  · HLS 스트림
  · 평균 응답시간
  · 확진 정상
  점검 필요를 카드로 요약 표시하고, 우측 상단에 증감(+/−)을 배지로 제공합니다.

- 상태:
  지표는 더미 값이며,백엔드 연동 시 실제 통계(CDC 읽기 모델)로 교체 예정입니다.

- CCTV 기기 상태(카드 그리드):
  · 각 장치 카드는 이름/자산번호·설치 주소, 실시간 썸네일(LIVE), 그리고 아래에 HLS 상태(정상/화질 저하 등) · 화질(720p/1080p) · 응답시간 · 마지막 점검일을 보여줍니다.

- 카드 하단 제어 버튼:
  · 일시정지/재시작/수리 요청 등이 배치되어 있으며 현재는 동작하지 않는 더미입니다.
  · 상태 배지로 정상/점검필요를 빠르게 식별할 수 있게 했습니다.

- 향후 구현 계획:
  · 정비 일정: 캘린더 기반 CRUD, 담당자 배정, 알림(슬랙/메일) 연동.
  · 정비 도구: 도구 재고/대여 이력 관리, 사용 주기 알림.
  · 장애 통계/리포트: 장치별 가용성, MTBF/MTTR, 화질·응답시간 트렌드.
  · 제어 버튼 실동작: 일시정지/재시작 API, 수리 티켓 생성(워크플로우 연동).

메모: 실제 배포 시, 이 대시보드는 프런트는 /api 상대경로, 백엔드는 권한(roles) 기반 인가로 기능 접근을 제어합니다.


| 장치 담당자 대시보드 |
|----------|
|![장치관리사+대시보드](https://github.com/user-attachments/assets/4cd9b91a-5cba-4c5a-9fd2-b291650e458c)|

<br>

## 8. 트러블 슈팅

- [백엔드 ↔ 프런트 연동 이슈 트러블슈팅](https://github.com/Kangarooseok/Prism/wiki/%EB%B0%B1%EC%97%94%EB%93%9C-%E2%86%94-%ED%94%84%EB%9F%B0%ED%8A%B8-%EC%97%B0%EB%8F%99-%EC%9D%B4%EC%8A%88-%ED%8A%B8%EB%9F%AC%EB%B8%94%EC%8A%88%ED%8C%85)

- [작성중](링크)

<br>

## 9. 개선 목표

    
- **25-09-17 개선 내용**
    
<br>

## 10. 프로젝트 후기

### 🍊 강민석

이번 프로젝트를 하면서 저는 관제의 핵심이 화면을 오래 보는 일이 아니라, 발생 → 판별 → 알림 → 기록이 끊기지 않고 이어지는 흐름을 만드는 데 있다는 것을 확실히 느꼈습니다. 이벤트와 기록이 차곡차곡 쌓이니 책임과 우선순위가 명확해졌고, 팀이 같은 그림을 보며 움직일 수 있었습니다.

또한 실시간성은 설계의 선택이라는 점을 배웠습니다. 배치를 스트림으로 바꾸자 지연이 구조적으로 줄었고, 조회를 분리한 만큼 스키마 변경과 재처리, 중복 처리 방지까지 책임 있게 운영 계획을 세워야 한다는 것도 깨달았습니다. 규칙은 단순할수록 유지보수가 쉬웠고, 역할 기반 화면과 /api 규칙 같은 작은 표준이 커뮤니케이션 비용을 크게 낮췄습니다. 아울러 비용 역시 중요한 요구 사항이어서, 저희 상황에서는 온프레미스와 클라우드를 함께 쓰는 방식이 더 현실적이라는 결론에 이르렀습니다.

처음에는 기능을 빨리 만드는 데만 집중했고, 장애는 운영 단계에서 자연스럽게 해결될 것이라고 생각했습니다. 프로젝트를 마치고 보니 기능은 이벤트와 기록으로 설명되어야 하며, 권한과 복구, 비용까지 포함해 하나의 서비스로 설계해야 한다는 관점으로 바뀌었습니다. 그래서 화면을 넘어서 감지 → 알림 → 이력의 전 과정을 제품으로 엮는 데 힘을 쏟았습니다.

<br>

### 👻 김시현

이번 프로젝트는 CCTV 관제 데이터를 이벤트 드리븐으로 재구성해, 쓰기 DB → Debezium → Kafka Connect JDBC Sink → 읽기 DB로 자동 동기화되는 단순·표준 파이프라인을 만들었습니다. 커스텀 컨슈머/프로듀서를 제거해 결합도를 낮추고 운영 복잡도를 줄였고, 상태감지 서비스는 읽기 DB에서 id·ip만 조회해 10초 주기로 ICMP/FFprobe 점검을 수행합니다. 결과는 health_metrics에 RTT(min/avg/max), 손실률, 상태 enum까지 저장하고, 룰 기반으로 8개 진단 코드를 산출해 알림을 자동 라우팅합니다. 또한 멀티 데이터소스 JPA 구성을 명확히 분리하고, 환경 설정을 로컬(.env)–Docker–K8s로 일관화해 배포·운영 안정성이 크게 올라갔습니다. 결론적으로 감지→진단→알림이 표준 컴포넌트로 자동화되어 구조를 단순화하고 신뢰도를 높일 수 있음을 분명히 깨달았습니다.

<br>

### 😎 최현제

Kafka와 Confluent Platform을 구축하면서 단순한 분산 메시징 시스템 설치를 넘어, 운영 안정성을 위한 세부 설정과 모니터링의 중요성을 깊이 체감할 수 있었습니다. 실제 장비와 AWS 아키텍처를 직접 다루며 인프라 전반에 대해 학습하는 과정은 제게 큰 즐거움이었고, 기술적 호기심을 더 키워준 시간이었습니다. 앞으로도 이러한 흥미와 열정을 회사 업무에도 이어가, 즐겁게 배우고 기여하는 엔지니어가 되고 싶습니다.

<br>

### 🐬 한선영

EDA, 분산환경에서의 마이크로서비스 아키텍처의 주의할 점을 공부했습니다. 서비스 도메인의 분리가 서비스 인프라에 직접적인 영향을 주었는데, 도메인 경계를 어떻게 나누느냐에 따라 네트워크 구성, 데이터베이스 분리, 배포 단위가 모두 달라지며 운영 복잡도가 크게 달라졌습니다. 또한 팀원과의 의견 충돌 시 단순히 기술적 타당성만으로는 합의가 어렵다는 것을 알게 되었고, 프로젝트의 목표와 팀 전체의 합의를 중심에 두고 논의를 이끌어가는 것이 중요하다는 교훈을 얻었습니다.

